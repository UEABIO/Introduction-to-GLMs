---
title: "Introduction to Generalised Linear Models"
subtitle: "ESA and Physalia Collaboration<br><small>4th March 2024</small>"
author: "Dr Philip Leftwich"
format:
  LUstyle-revealjs:
    self-contained: true
    auto-stretch: false
    footer: "{{< fa envelope >}} [p.leftwich@uea.ac.uk](mailto: p.leftwich@uea.ac.uk) {{< fa globe >}} [philip.leftwich.github.io](https://philip.leftwich.github.io/) {{< fa brands linkedin >}} [philip-leftwich](https://www.linkedin.com/in/philip-leftwich-117052155/)"
---

# Welcome! {background-color="#D9DBDB"}

```{r}
#| include: false
#| message: false
#| warning: false

library(arm)
library(car)
library(DHARMa)
library(emmeans)
library(equatiomatic)
library(fitdistrplus)
library(gtsummary)
library(lmtest)
library(MASS)
library(MuMIn)
library(performance)
library(pscl)
library(tidyverse)
library(sjPlot)


```

::: columns
::: {.column .right}


:::

::: {.column}

![](images/physalia.png){fig-align="center" fig-alt="physalia logo" width=70%}

:::
:::


## About me

::: columns
::: {.column .right}

Associate Professor in Data Science and Genetics at the [University of East Anglia](https://www.uea.ac.uk/).

<br>

Academic background in Behavioural Ecology, Genetics, and Insect Pest Control.

<br>

Teach Genetics, Programming, and Statistics

:::

::: {.column}

![](images/UEA.jpg){fig-align="center" fig-alt="UEA logo" width=70%}

:::
:::

## About me

::: columns
::: {.column .right}


Norwich - "A Fine City!"

![](images/norwich.jpeg){fig-align="center" fig-alt="Norwich cathedral" width=90%}

:::

::: {.column}

![](images/norwich_map_position_in_uk_000001.png){fig-align="center" fig-alt="Norwich on a map" width=70%}

:::
:::

# Introduce yourselves {background-color="#D9DBDB"}


## Outline of the course

- Why be normal? 

- GLM with binary data

- GLM with count data

## What to expect during this workshop

These workshop will run for *4 hours* each.

We will have a 30 min break halfway through

* Combines slides, live coding examples, and exercises for you to participate in.

* Ask questions in the chat throughout!

## What to expect during this workshop

::: columns
::: {.column}

<br>

I hope you end up with more questions than answers after this workshop!

:::

::: {.column .center-text}

<br>

![](images/great-question.gif){fig-align="center" fig-alt="Schitts Creek questions gif" width=60%}

<small>Source: <a href="https://giphy.com/gifs/schittscreek-64afibPa7ySzhFAf00">giphy.com</a></small>

:::
:::

## What you need for this workshop

* You are comfortable with simple operations in R.

* You know how to perform linear regression.

* You want to learn some **Generalised** Linear Models


## Workshop resources

Suggested R packages: 

```{r}
#| eval: false
#| echo: true


library(arm)
library(car)
library(DHARMa)
library(emmeans)
library(equatiomatic)
library(fitdistrplus)
library(gtsummary)
library(lmtest)
library(MASS)
library(MuMIn)
library(performance)
library(pscl)
library(tidyverse)
library(sjPlot)


```


## Workshop Resources

<br><br>

- Slides available [here](https://ueabio.github.io/Introduction-to-GLMs/)

- Data available [here](https://github.com/UEABIO/Introduction-to-GLMs/tree/scripts/data)

- Scripts available [here](https://github.com/UEABIO/Introduction-to-GLMs/tree/scripts/scripts)

- Posit Cloud workspace available [here](https://posit.cloud/spaces/487933/join?access_code=vcIMD3l3DmNdEZkqVJAocY1xJQlfqTd2r_9adPH1)


# General linear models {background-color="#D9DBDB"}

## Recapping general linear models

- Also known at the Ordinary Least Squares (OLS) model

- It aims to fit a regression line which minimises the squared differences between observed and predicted values

:::{.fragment}

![](images/ols.png){fig-align="center" fig-alt="ordinary least squares" width=70%}

:::

## Equation of the line

The equation of a linear model (lm) is given by:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where:

:::{.incremental}

- $y_i$ is the predicted value of the response variable.

- $\beta_0$ is the intercept.

- $\beta_1$ is the slope coefficient, representing the change in the response variable for a one-unit change in the explanatory variable.

- $x_i$ is the value of the explanatory variable.

- $\epsilon_i$ represents the residuals of the model
:::



## Limitations of general linear models

**Q**. What are the assumptions of a general linear model?

::: {.incremental}

* Assumes a linear relationship

* Assumes normal distribution of the residuals

* Assumes homogeneity of the residuals

* Independence - assumes observations are independent of each other

:::



## Normal distribution of the residuals

The linear regression line is the most likely line given your data if we assume each data point comes from a hypothetical bell curve centered on the regression line

![](images/prob-reg.jpeg){fig-align="center" fig-alt="probability" width=60%}

## Normal distribution

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| layout-ncol: 2
#| message: false

# Define a range of mean and standard deviations (average rate of success)
mean <- rep(c(10,20,30), 3)
sd <- rep(c(5, 7, 10), each = 3)

# Generate the data
norm_data <- map2_df(mean, sd, ~tibble(
  mean = factor(.x),
  sd = factor(.y),
  x = seq(0,40, length.out = 100),
  density = dnorm(x, .x, .y)
))  # For ordered plotting

# Plot
norm_data |> 
  filter(mean == 20) |> 
ggplot(aes(x = x, y = density, color = sd)) +
  geom_line(linewidth = 1.5) +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Change in Normal Distribution \nwith different Standard Deviations, Mean = 20",
       x = "X",
       y = "Probability",
       color = "SD") +
  theme_minimal(base_size = 16)

norm_data |> 
  filter(sd == 7) |> 
ggplot(aes(x = x, y = density, color = mean)) +
  geom_line(linewidth = 1.5) +
  scale_color_brewer(palette = "Accent") +
  labs(title = "Change in Normal Distribution \nwith different Means, SD = 7",
       x = "X",
       y = "Probability",
       color = "Mean") +
  theme_minimal(base_size = 16)

```


## Normal distribution of the residuals

When using our residuals to calculate standard error, confidence intervals and statistical significance these are assumed to be drawn from:

- a normal distribution with mean zero 

- with a constant variance. 

:::{.fragment}
<br>
This implies that the residuals, or the distances between the observed values and the values predicted by the linear model, can be modeled by drawing random values from a normal distribution.

:::

## The linear model equation

Another way to write the lm equation is:

$$
y_i \sim N(\mu = \beta_0 + \beta_1 X_i, \sigma^2)
$$

:::{.fragment}

Which literally means that $y_i$ is drawn from a normal distribution with parameters:

- $\mu$ (which depends on $x_i$)  

- $\sigma$ (which has the same value for all measures of $Y$).

:::


## Real data

How often do these assumptions really hold true? 

A good fit: 

```{r}
#| eval: true
#| echo: false
#| layout-ncol: 2
#| fig-height: 7
#| message: false


janka <- readr::read_csv("../data/janka.csv")

model <- lm(sqrt(hardness) ~ dens, weights = 1/sqrt(hardness), data = janka)

plot(model, which=2)

plot(model, which=3)

```


## Real data

How often do these assumptions really hold true? 

An ok fit: 

```{r}
#| eval: true
#| echo: false
#| layout-ncol: 2
#| fig-height: 7
#| message: false

cuckoo <- read_csv("../data/cuckoo.csv")

cuckoo_lm <- lm(Beg ~ Mass + Species + Mass:Species, data = cuckoo)

plot(cuckoo_lm, which=2)

plot(cuckoo_lm, which=3)
```

## Real data

How often do these assumptions really hold true? 

A poor fit: 

```{r}
#| eval: true
#| echo: false
#| layout-ncol: 2
#| fig-height: 7
#| message: false

load(file = "../data/Mayflies.rda")

Mayflies_lm <- lm(Occupancy ~ CCU, data = Mayflies)

plot(Mayflies_lm, which=2)

plot(Mayflies_lm, which=3)
```


## Assumption testing


```{r}
#| fig-height: 7
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot(model)

```




## Residuals vs Fitted

::::{.columns}

:::{.column}

```{r}

plot(model, which=1)

```

:::

:::{.column}

- **Purpose:** To check for linearity and homoscedasticity.

- **Interpretation:** Look for a random scatter of points around the horizontal line at y = 0. A funnel-shaped pattern suggests heteroscedasticity.

:::

::::


## QQ-plot

::::{.columns}

:::{.column}

```{r}

plot(model, which=2)

```

:::

:::{.column}

- **Purpose:** To assess the normality of the residuals

- **Interpretation:** Points should fall along the diagonal line. Deviation from the line indicates non-normality of residuals.

:::

::::

## Scale-location

::::{.columns}

:::{.column}

```{r}

plot(model, which=3)

```

:::

:::{.column}

- **Purpose:** To check homoscedasticity and identify outliers

- **Interpretation:** Uses **standardised** residuals. Constant spread indicates homoscedasticity

:::

::::

## Residuals vs. Leverage

::::{.columns}

:::{.column}

```{r}

plot(model, which=4)

```

:::

:::{.column}

- **Purpose:** To identify influential data points(outliers)

- **Interpretation:** Loking for points with high leverage that might affect the regression line. Investigate values > 0.5

:::

::::


## Performance

::::{.columns}

:::{.column}

The `performance` package from [easystats](https://easystats.github.io/easystats/) can produce (among other things) similar plots.

:::

:::{.column}

```{r}
#| fig-height: 7

library(performance)
check_model(model)

```

:::

::::

## Formal tests

::::{.columns}

:::{.column}

- For linear models we can also run some formal tests on the residuals

```{r}
#| eval: true
#| echo: true
#| message: false

# lmtest::bptest(model)

performance::check_normality(model)

# shapiro.test(residuals(model))

performance::check_heteroscedasticity(model)

```
:::

:::{.column}

- This becomes more difficult with generalised models

- Being able to interpret residual plots is important

:::

::::

# Generalised Linear Models {background-color="#D9DBDB"}

## What are Generalised linear models?

:::{.incremental}

- The Generalised Linear Model (GLM) is an extension of the ordinary linear regression model that accommodates a broader range of response variable types and error distributions.

- Key components: 

- **Linear Predictor:** Combines predictor variables linearly.

- **Link Function:** Links the mean of the response variable to the linear predictor.

- **Error family:** Drawn from the **exponential family** it determines the shape of the response variable's distribution.

:::


## Linear Predictor

Linear models:

::: {.center-text}


$y_i = \beta0 + \beta_1x1 + \beta_2x2 + ... +\beta_nxn +\epsilon_i$

:::



::: {.fragment}

<br>

Then generalised linear models...

::: {.center-text}

$\eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_n x_{ni}$


:::

- $\eta_i$ the linear prediction for observation $i$

:::


## The link function


Let's start with linear models...

::: {.center-text}


$y_i = \beta0 + \beta_1x1 + \beta_2x2 + ... +\beta_nxn +\epsilon_i$

:::

::: {.fragment}

<br>

Then generalised linear models...

::: {.center-text}

$g(\mu_i) = \eta_i$


:::

:::

:::{.fragment}

- $\eta_i$ linear prediction for observation $i$

- $g$ the link function relating the linear predictor to the expected value (mean) of the response variable

- $\mu_i$ is the expected value of the response variable for observation $i$

Common link functions: identity, sqrt, log, logit

:::


## Error family

Let's start with linear models...

::: {.center-text}


$y_i = \beta0 + \beta_1x1 + \beta_2x2 + ... +\beta_nxn +\epsilon_i$


$\epsilon_i = \mathcal{N}(0, \sigma^2)$

:::



::: {.fragment}

Then generalised linear models...

::: {.center-text}

$\mu_i = f(\eta_i)$

$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

:::

:::

:::{.fragment}

 - $\epsilon_i$ is the error term for observation $i$

**Examples:** Gaussian (normal), binomial, Poisson, gamma, etc.

:::

## Generalised Linear Model

::::{.columns}

:::{.column}

**Linear Predictor** ($\eta$):

$\eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots$

**Link Function** ($g$):

$g(\mu_i) = \eta_i$

**Probability Distribution**:

$\mu_i = f(\eta_i)$

$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

:::

:::{.column}

- $\eta_i$ linear prediction for observation $i$

- $g$ the link function relating the linear predictor to the expected value (mean) of the response variable

- $\mu_i$ is the expected value of the response variable for observation $i$

- $f$ the inverse of the link function, transforming the linear predictor back to the response variable's expected value

- $\epsilon_i$ is the error term for observation $i$

:::

::::

## Changing model structures

There are a range of model structures available. 

The choice of error family and link influence how well the model fits

| Exponential Error Family      | Link Function       |
|-------------------|---------------------|
| Gaussian          | Identity, sqrt, log, inverse            |
| Binomial          | Logit, probit, cloglog               |
| Poisson           | Log, identity, sqrt                 |
| Gamma             | Inverse, identity, log, sqrt             |
| Negative Binomial | Log, identity, sqrt|


# Questions? {background-color="#D9DBDB"}

# A Linear Model Example {background-color="#D9DBDB"}

## Janka timber hardness

This regression analysis uses data from the Australian forestry industry recording wood density and timber hardness from 36 different samples

:::{.incremental}

- Timber hardness is quantified on the "Janka" scale

- The 'amount of force required to embed a 0.444" steel ball into the wood to half of its diameter'.

:::

## Fitting the general linear model

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false

janka <- readr::read_csv("../data/janka.csv")

```


```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_ls <- lm(hardness ~ dens, data = janka)

summary(janka_ls)

janka |> 
  ggplot(aes( x = dens, y = hardness))+
  geom_point()+
  geom_smooth(method = "lm")

```


## Challenge: Fit a Linear Model

<br><br>

* Fit the Linear Model `lm(hardness ~ dens, data = janka)` to the janka data.

* Evaluate the model fit

```{r}
#| label: ex-lm-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```


## Model diagnostics

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

plot(janka_ls, which=2)

plot(janka_ls, which=3)
```


## Violations of assumptions

```{r}
#| eval: true
#| echo: true
#| fig-height: 8
#| message: false

library(lmtest)

# Breusch-Pagan Test of Homoscedasticity
lmtest::bptest(janka_ls)

# Shapiro-Wilk test for normality of residuals
shapiro.test(residuals(janka_ls))



```


## Challenge

<br><br>

* What can we try to improve the fit of our `lm(hardness ~ dens, data = janka)` ?

<br>

* Let's make suggestions then try them out:

```{r}
#| label: ex-transform-lm-timer
countdown::countdown(
  minutes = 15,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```


## BoxCox

The R output for the `MASS::boxcox()` function plots a maximum likelihood curve (with a 95% confidence interval - drops down as dotted lines) for the best transformation for fitting the dependent variable to the model.

::::{.columns}

:::{.column}

| lambda value | transformation |
|--------------|----------------|
| 0            | log(Y)         |
| 0.5          | sqrt(Y)        |
| 1            | Y              |
| 2            | Y^1            |

:::

:::{.column}

```{r}
#| eval: true
#| echo: true
#| fig-height: 7

MASS::boxcox(janka_ls)

```

:::

::::

## Square root transformation

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_sqrt <- lm(sqrt(hardness) ~ dens, data = janka)

plot(janka_sqrt, which=2)

plot(janka_sqrt, which=3)

```

## Model fit

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

ggplot(janka, aes(x = hardness, y = dens)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "lm", formula = y ~ sqrt(x)) +  # regression line
  labs(title = "Sqrt Linear Regression with ggplot2",
       x = "X", y = "Y")  # axis labels and title

```


## Log transformation

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_log <- lm(log(hardness) ~ dens, data = janka)

plot(janka_log, which=2)

plot(janka_log, which=3)

```

## Model fit

```{r}
#| eval: true
#| echo: true
#| fig-height: 8
#| message: false
ggplot(janka, aes(x = hardness, y = dens)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "lm", formula = (y ~ log(x))) +  # regression line
  labs(title = "Log Linear Regression",
       x = "X", y = "Y")  # axis labels and title
```

## Polynomials

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_poly <- lm(log(hardness) ~ poly(dens, 2), data = janka)

plot(janka_poly, which=2)

plot(janka_poly, which=3)

```

## Polynomial fit

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

summary(janka_poly)



ggplot(janka, aes(x = hardness, y = dens)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "lm", formula = (y ~ poly(log(x), 2))) +  # regression line
  labs(title = "Quadratic Log Linear Regression",
       x = "X", y = "Y")  # axis labels and title



```


## Weighted least squares

```{r}
#| eval: true
#| echo: true
#| message: false

janka_wls <- lm(sqrt(hardness) ~ dens, weights = 1/sqrt(hardness), data = janka)

```

- Weights are specified as $\frac{1}{\sqrt{\text{hardness}}}$, indicating that observations with higher hardness values will have lower weights, and vice versa.

- This weighting scheme can assign more importance to certain observations in the model fitting process.

- This approach is known as weighted least squares (WLS) regression, where observations are weighted differently based on certain criteria, such as the square root of hardness.

## Weighted least squares

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_wls <- lm(sqrt(hardness) ~ dens, weights = 1/sqrt(hardness), data = janka)

plot(janka_wls, which=2)

plot(janka_wls, which=3)

```

## Model fit

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| layout-ncol: 2
#| message: false

prediction_data <- data.frame(dens = sort(unique(janka$dens)))
predictions <- predict(janka_wls, newdata = prediction_data, interval = "confidence", level = 0.95)

# Adding predictions and confidence intervals to the dataframe
prediction_data$wls_pred = predictions[, "fit"]
prediction_data$wls_pred.lwr = predictions[, "lwr"]
prediction_data$wls_pred.upr = predictions[, "upr"]

ggplot(janka) +
     geom_ribbon(data = prediction_data, aes(x = dens, ymin = wls_pred.lwr, ymax = wls_pred.upr), alpha = 0.8, fill = "lightgray")+
    geom_line(data = prediction_data, aes(x = dens, y = wls_pred), color = "blue")+
  geom_point(aes(x = dens, y = sqrt(hardness)))

summary(janka_wls)

```

## Troublesome transformations

:::{.incremental}

- Changes interpretability

- Can introduce NEW violations of assumptions

- Error and Mean change together

- We don't understand the true structure of our data

:::


# A Generalised Linear Model Example {background-color="#D9DBDB"}

## A Generalised linear model approach


* Fit a `glm(gaussian(link = "identity"))` to the data

* Check that the model is identical to the `lm()`


```{r}
#| label: ex-glm-timer
countdown::countdown(
  minutes = 5,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

```{r}
#| eval: true
#| echo: true
#| fig-height: 8
#| message: false


janka_glm <- glm(hardness ~ dens, data = janka, family = gaussian(link = "identity"))

summary(janka_glm)

```

## A Generalised linear model approach

We previously identified that the **square root** of the dependent variable might have been a better fit with our linear model:

```{r}
#| eval: true
#| echo: true
#| fig-height: 8
#| message: false


janka_glm <- glm(hardness ~ dens, data = janka, family = gaussian(link = "sqrt"))

summary(janka_glm)

```


## Plot model

::::{.columns}

:::{.column}

```{r}
#| eval: false
#| echo: true
#| fig-height: 8
#| message: false


ggplot(janka, aes(x = dens, y = hardness)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "glm", method.args = list(gaussian(link = "sqrt"))) +  # regression line
  labs(title = "Linear Regression with ggplot2",
       x = "X", y = "Y")  # axis labels and title


```

:::

:::{.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false


ggplot(janka, aes(x = dens, y = hardness)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "glm", method.args = list(gaussian(link = "sqrt"))) +  # regression line
  labs(title = "Linear Regression with ggplot2",
       x = "X", y = "Y")  # axis labels and title


```

:::

::::

## Residuals

```{r}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| fig-height: 8
#| message: false

janka_glm <- glm(hardness ~ dens, data = janka, family = gaussian(link = "sqrt"))

plot(janka_glm, which=2)

plot(janka_glm, which=3)

```


## Choosing an Error Family

::::{.columns}

:::{.column}

```{r}
#| eval: true
#| echo: true
#| fig-height: 7
#| message: false

library(fitdistrplus)

descdist(janka$hardness, boot = 500)

```

:::

:::{.column}

- A Cullen and Frey plot (skewness-kurtosis)

- A graphical view of the distribution of the dependent variable

- Reference lines represent expected skew and kurtosis for different distributions

- Beta family sits outside of GLM exponential families

:::

::::

## Choosing an Error family

::::{.columns}

:::{.column}

```{r}
#| eval: false
#| echo: true
#| fig-height: 8
#| message: false
#| warning: false

library(fitdistrplus)

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fg <- fitdist(janka$hardness, "gamma")
fn <- fitdist(janka$hardness, "norm")
plot.legend <- c("normal", "gamma")
denscomp(list(fn, fg), legendtext = plot.legend)
qqcomp(list(fn, fg), legendtext = plot.legend)
```

:::

:::{.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false
#| warning: false

library(fitdistrplus)

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fg <- fitdist((janka$hardness), "gamma")
fn <- fitdist((janka$hardness), "norm")
plot.legend <- c("normal", "gamma")
denscomp(list(fn, fg), legendtext = plot.legend)
qqcomp(list(fn, fg), legendtext = plot.legend)
```

:::

::::

## Choosing based on AIC

::::{.columns}

:::{.column}

```{r}
#| eval: true
#| echo: true
#| message: false

summary(fg)
```

:::

:::{.column}

```{r}
#| eval: true
#| echo: true
#| message: false

summary(fn)
```

:::

::::




## Gamma

:::: columns

::: {.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false

# Define a range of shape values
shape_values <- c(1, 2, 5, 10)
scale_value <- 2 # Keep scale fixed for simplicity

# Generate the Gamma distribution data
gamma_data <- map_df(shape_values, ~tibble(
  shape = .x,
  x = seq(0, 20, length.out = 100),
  density = dgamma(seq(0, 20, length.out = 100), shape = .x, scale = scale_value)
)) %>%
  mutate(shape = factor(shape, levels = shape_values)) # For ordered plotting

# Plot
ggplot(gamma_data, aes(x = x, y = density, color = shape)) +
  geom_line() +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Change in Gamma Distribution with Different Shape Parameters",
       x = "Value",
       y = "Density",
       color = "Shape Parameter") +
  theme_minimal(base_size = 16)

```

$f(x;\alpha,\beta) = \frac{\beta^\alpha x^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)}$

::: 

::: {.column}

- Shape (α): Determines the shape or curve of the distribution. Higher values of α result in distributions that are more peaked and skewed to the left.

- Rate (β): Influences the spread of the distribution. Smaller values of β result in distributions that are more spread out, larger values of β lead to distributions that are less spread out.

- Useful for modelling **positively skewed continuous  non-negative data**

:::


::::


## Gamma GLM

```{r}
#| eval: true
#| echo: true
#| fig-height: 8
#| message: false


janka_gamma <- glm(hardness ~ dens, data = janka, family = Gamma(link = "sqrt"))

summary(janka_gamma)

```

## Plot model

::::{.columns}

:::{.column}

```{r}
#| eval: false
#| echo: true
#| fig-height: 8
#| message: false


ggplot(janka, aes(x = dens, y = hardness)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "glm", method.args = list(Gamma(link = "sqrt"))) +  # regression line
  labs(title = "Linear Regression with ggplot2",
       x = "X", y = "Y")  # axis labels and title


```

:::

:::{.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false


ggplot(janka, aes(x = dens, y = hardness)) +
  geom_point() +  # scatter plot of original data points
  geom_smooth(method = "glm", method.args = list(Gamma(link = "sqrt"))) +  # regression line
  labs(title = "Linear Regression with ggplot2",
       x = "X", y = "Y")  # axis labels and title


```

:::

::::

## Exercise: Gamma

* Fit the Gamma distribution linear model to the data.

* `glm(hardness ~ dens, data = janka, family = Gamma(link = "sqrt")`

* Evaluate the model fit against other glm fits.

```{r}
#| label: ex-gamma-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```



# Questions? {background-color="#D9DBDB"}


# Likelihood {background-color="#D9DBDB"}


## Log-Likelihood

::::{.columns}

:::{.column}

- Likelihood measures the probability of observing the given data under a specific statistical model, as a function of the model's parameters.

- It represents how well the model explains the observed data.

- The log likelihood function in maximum likelihood estimations is usually computationally simpler as it is additive.

:::

:::{.column}

![](images/log-likelihood.png){fig-align="center" fig-alt="The log-likelihood (l) maximum is the same as the likelihood (L) maximum." width=100%}

:::

::::

## Log-Likelihood

Likelihood Function:

$L(\theta | y) = f(y_1 | \theta) \times f(y_2 | \theta) \times \ldots \times f(y_n | \theta)$

:::{.fragment}
<br>
Log-Likelihood Function:

$\ell(\theta | y) = \log(f(y_1 | \theta)) + \log(f(y_2 | \theta)) + \ldots + \log(f(y_n | \theta))$

:::

:::{.fragment}
<br>
Although log-likelihood functions are mathematically easier than their multiplicative counterparts, they can be challenging to calculate by hand. They are usually calculated with software.

:::

## Maximum Likelihood Estimation

For a specified error family distribution and link function, the glm finds parameter values that have the highest likelihood score.

:::{.incremental}

- Which estimates are most probable for producing the observed data

- This is an iterative process, without a single defined equation

- For gaussian and identity link it will produce the same outcome as OLS.

:::

Log-Likelihood Function:
$$\ell(\theta | \mathbf{y}) = \sum_{i=1}^{n} \log(f(y_i | \theta))$$


## Maximum Likelihood


::::{.columns}

:::{.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false
#| warning: false

# Simulated data
set.seed(42)

log_likelihood_visual <- function(mean = 6, sd = 1){

data <- rnorm(100, mean = mean, sd = sd)

# Define a grid of mu and sigma values
mu_range <- seq(mean-(sd*2), mean+(sd*2), length.out = 100)
sigma_range <- seq(sd-sd, sd+sd, length.out = 100)
sigma_range[sigma_range < 1] <- 1
grid <- expand.grid(mu = mu_range, sigma = sigma_range)

# Function to calculate log-likelihood for normal distribution
log_likelihood <- function(mu, sigma, data) {
  n <- length(data)
  -n/2 * log(2 * pi) - n * log(sigma) - 1/(2 * sigma^2) * sum((data - mu)^2)
}

# Calculate log-likelihood for each combination of mu and sigma
grid$log_likelihood <- mapply(log_likelihood, mu = grid$mu, sigma = grid$sigma, MoreArgs = list(data = data))

# Plot
ggplot(grid, aes(x = mu, y = sigma, z = log_likelihood)) +
  geom_contour_filled(aes(fill = after_stat(level)), bins = 20) + # Use geom_contour_filled for filled contour plots
  labs(title = "Log-Likelihood Contour Plot",
       x = expression(mu),
       y = expression(sigma),
       fill = "Log-Likelihood") +
  theme_minimal()
}

log_likelihood_visual()
```

:::

:::{.column}

- A contour plot visualizing the log-likelihood surface for a normal distribution fitted to simulated data. 

- The contour lines on the plot represent regions of equal log-likelihood values. Darker regions indicate lower log-likelihood values, while lighter regions indicate higher log-likelihood values

:::

::::

## Exercise: log-likelihood

<br><br>

* Use the log-likelihood function with varying values of the mean and sd

```{r}
#| label: ex-log-timer
countdown::countdown(
  minutes = 5,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

## Deviance

- The deviance is a key concept in Generalised linear models. 

- It measures the deviance of the fitted Generalised linear model with respect to a perfect/**saturated** model for the sample.

```{r}
#| eval: true
#| echo: false
#| fig-height: 6
#| message: false



# Simulate some data
set.seed(123)
n <- 100  # Sample size
x <- rnorm(n)  # Predictor variable
y <- 2 + 3*x + rnorm(n)  # Continuous response variable (linear relationship)

# Fit the null model (intercept only)
null_model <- lm(y ~ 1)

# Fit the saturated model (fully saturated model)
saturated_model <- lm(y ~ x)

# Fit the fitted model (linear predictor with x)
fitted_model <- lm(y ~ x)

# Create a dataframe to store observed points
observed_data <- data.frame(x = x, y = y)

# Generate fitted lines
fitted_lines <- data.frame(x = sort(x))
fitted_lines$Null <- predict(null_model, newdata = fitted_lines)
fitted_lines$Fitted <- predict(fitted_model, newdata = fitted_lines)

# Plot observed points and fitted lines
ggplot() +
    geom_point(data = observed_data, aes(x = x, y = y), color = "black") +
    geom_line(data = fitted_lines, aes(x = x, y = Null, color = "Null")) +
    geom_line(data = observed_data, aes(x = x, y = y, color = "Saturated")) +
    geom_line(data = fitted_lines, aes(x = x, y = Fitted, color = "Fitted")) +
    scale_color_manual(values = c("Null" = "red", "Saturated" = "blue", "Fitted" = "green"),
                       name = "Model") +
    labs(
        title = "Fitted Lines vs Observed Points",
        x = "x",
        y = "y"
    ) +
    theme_minimal(base_size = 14)


```




## Pseudo R-squared

::::{.columns}

:::{.column}

![](images/deviance.png){fig-align="center" fig-alt="" width=100%}

:::

:::{.column}

Formula: 

$GLM~R^2 = 1 - \frac{D}{D_0}$

$linear\_model~R^2= 1 - \frac{SSE}{SST}$


- $D$ is the deviance of the fitted model.

- $D_0$ is the deviance of the null model (linear model with only an intercept).

The GLM $R^2$ **is not** the percentage of variance explained by the model, but rather a ratio indicating how close the fit is to being perfect.

:::

::::

## Pseudo R-squared in R

- We can fit the deviance-based pseudo $R^2$ with the `MuMIn` package

- `r.squaredLR()` is a function used to compute the likelihood ratio (LR) based R-squared for a given model

```{r}
#| eval: true
#| echo: true
#| message: false



library(MuMIn)

r.squaredLR(janka_gamma)


```

## AIC

:::{.incremental}

- The Akaike Information Criterion (AIC)  balances the trade-off between model fit and complexity by penalizing models with more parameters.

- It aims to find the model that best explains the data while avoiding overfitting.

- AIC is calculated as follows: $\text{AIC} = 2k - 2\ln(\hat{L})$, 

- where: log-likelihood is the maximized value of the likelihood function for the fitted model. $k$ is the number of parameters in the model.

- In general a difference in AIC of <2 suggests equivalent goodness-of-fit

:::

## AIC

::::{.columns}

:::{.column}

**AIC can:**

- Compare non-nested models fitted to the same dataset and response variable

- Compare across different GLM families and link functions

:::

:::{.column}

**AIC cannot:**

- Compare models where the response variable is transformed

- Compare models where there are differences in the dataset

:::

::::

## Exercise: Compare models

* Revisit the janka models

* Compare R-squared fits and AIC values

* `r.squaredLR()` and `AIC()`

```{r}
#| label: ex-fits-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```



## Likelihood ratio test

The likelihood ratio test (LRT) “unifies” frequentist statistical tests. Brand-name tests like t-test, F-test, chi-squared-test, are specific cases (or even approximations) of the LRT.

- The likelihood ratio test (LRT) begins with a comparison of the likelihood scores of the two models:

$\text{LR} = -2(\ln L_1 - \ln L_2)$

:::{.fragment}

Where:

- $\ln L_1$ and $\ln L_2$ are the log-likelihoods of the two models being compared.

:::

## Likelihood ratio test

- This LRT statistic approximately follows a chi-square distribution. 

- To determine if the difference in likelihood scores among the two models is statistically significant, we next must consider the degrees of freedom. 

- In the LRT, degrees of freedom is equal to the number of additional parameters in the more complex model.

```{r}
#| echo: false
#| eval: true

fitted_model <- glm(hardness ~ dens, data = janka, family = Gamma(link = "sqrt"))
null_model <- glm(hardness ~ 1, data = janka, family = Gamma(link = "sqrt"))
fitted_likelihood <- logLik(glm(hardness ~ dens, data = janka, family = Gamma(link = "sqrt")))
null_likelihood <- logLik(glm(hardness ~ 1, data = janka, family = Gamma(link = "sqrt")))
```

```{r}
#| echo: true
#| eval: true


LR <- -2*(null_likelihood[1]-fitted_likelihood[1])

LR

```

## Likelihood ratio test

:::{.incremental}

- The likelihood ratio test `lrtest()` is typically calculated as twice the difference in deviance between the two models.

- Scaling by -2 ensures the test statistic follows a chi-square distribution under certain conditions

- We can calculate the likelihood ratio test statistic, compare it to the chi-square distribution with the appropriate degrees of freedom, and determine whether the improvement in fit between the two models is statistically significant.

:::

```{r}

lrtest(null_model, fitted_model)

```

## F distribution

What if the variance is unknown e.g. Gaussian and Gamma distributions?

:::{.incremental}

- Then the null model estimates one parameter while the alternative model estimates two, so the difference in df is still 1. 

- In this case, we know this more familiarly as the ANOVA or F-test, which in this example is equivalent to the t-test on the intercept.

:::

```{r}
summary(fitted_model)

anova(null_model, fitted_model, test = "F")

 drop1(fitted_model, test = "F")

```

## F distribution

What if the variance is unknown e.g. Gaussian and Gamma distributions?


```{r}
#| echo: true
#| eval: true

anova(null_model, fitted_model, test = "F")

# drop1(fitted_model, test = "F")

```


## Challenge: Fruitfly


* Using the `fruitfly.csv` dataset

* Build a model to investigate the effect of sleep, mating status and body size on longevity

* Check the model fits and report findings

```{r}
#| label: ex-fly-glm-timer
countdown::countdown(
  minutes = 30,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

## Confidence intervals

```{r}
#| echo: false
#| eval: true

fruitfly <- read_csv("../data/fruitfly.csv")

fly_gamma <- glm(longevity ~ type + thorax + sleep, family = Gamma(link = "identity"), data = fruitfly)

```

```{r}
#| echo: true
#| eval: true
#| message: false

# Assuming 'model' is your fitted GLM model

# Extract coefficients and confidence intervals
coef_ci <- confint(fly_gamma)

# Extract estimates
estimates <- coef(fly_gamma)

# Combine estimates with confidence intervals
estimates_df <- data.frame(
  estimates = estimates,
  lower_ci = coef_ci[,1],
  upper_ci = coef_ci[,2]
)

# Display dataframe
estimates_df

```

## Confidence intervals

```{r}
#| echo: true
#| eval: true
#| message: false

# Alternatively, using package-specific functions
# Example with 'broom' package
library(broom)
tidy(fly_gamma, conf.int = T)

```

## Extracting main effects

- Estimates and confidence intervals are set agains the intercept

- Hypothesis testing of factors with more than one level requires lrt:

```{r}
#| echo: true
#| eval: true
#| message: false

drop1(fly_gamma, test = "F")

```

## Estimates and post-hoc

The `emmeans` package in R computes estimated marginal means (EMMs) it can also provides post-hoc comparisons and contrasts for fitted models.

By default the package computes marginal means with 95% confidence intervals for the average of each variable

```{r}
#| echo: true
#| eval: true
#| message: false

emmeans::emmeans(fly_gamma, 
                 specs = pairwise ~ type + thorax + sleep,
                 type = "response")

```

## Tables

- There are multiple packages available that provide ways to present publication ready model summaries

```{r}
#| echo: true
#| eval: false
#| message: false

library(gtsummary)

tbl_regression(fly_gamma)

library(sjPlot)

tab_model(fly_gamma)

```

## Summary table & Figures


```{r}
#| echo: false
#| eval: true
#| message: false
#| layout-ncol: 2
#| fig-height: 8

library(gtsummary)

tbl_regression(fly_gamma)


# Co-erce emmeans to a tibble  - provide range of required predictions for thorax 
means <- emmeans::emmeans(fly_gamma, 
                 specs = ~ type + thorax,
                 at = list(thorax=c(0.64,0.94)),
                 type = "response") |> 
  as_tibble()

# set figure colours
colours <- c("cyan", "darkorange", "purple")

# Overly model predictions with observed values
fruitfly |> 
  ggplot(aes(x=thorax, y = longevity, colour = type, fill = type))+
  geom_ribbon(data = means,
              aes(x = thorax, 
                  y = emmean,
                  ymin=lower.CL, 
                  ymax=upper.CL), alpha = 0.2)+
  geom_line(data = means,
            aes(linetype = "dashed",
                x = thorax,
                y = emmean),
                show.legend = FALSE)+
  geom_point(data = fruitfly, aes(x = thorax, y = longevity),
             show.legend = FALSE)+
  scale_colour_manual(values = colours)+
  scale_fill_manual(values = colours)+
  labs(y = "Lifespan in days",
       x = "Thorax length (mm)",
       fill = "Type of female exposure")+
  guides(colour = "none")+
  theme_classic(base_size = 16)
```



# GLMs for binary data {background-color="#D9DBDB"}

## GLMS for binary data 

Common response variable in ecological datasets is the binary variable: we observe a phenomenon X or its “absence”

:::{.incremental}

* Presence/Absence of a species

* Presence/Absence of a disease

* Success/Failure to observe behaviour

* Survival/Death of organisms

:::

Wish to determine if $P/A∼ Variable$

Called a logistic regression or logit model

## Binary variables

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false
#| warning: false

load(file = "../data/Mayflies.rda")

```


:::{.incremental}

- Investigating mayfly abundance in relation to pollution gradient measured in Cumulative Criterion Units (CCU)

- Mayflies serve as indicators of stream health due to sensitivity to metal pollution

- Higher CCU values indicate increased metal pollution, potentially impacting mayfly presence or absence in the stream

:::

## Example

Fitting a linear regression vs a logit-link regression:

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| layout-ncol: 2
#| message: false
#| warning: false

ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()+
  geom_smooth(method = "lm")+
    labs(title = "Linear Regression",
       x = "CCU", y = "Occupancy") +
  theme_classic(base_size = 14)

ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()+
  geom_smooth(method = "glm", method.args = list(binomial(link = "logit"))) +
  labs(title = "Logit-link Binomial Regression",
       x = "CCU", y = "Occupancy") +
  theme_classic(base_size = 14)

```



## Bernoulli Distribution

::::{.columns}

:::{.column}

The Bernoulli distribution models the probability of success or failure in a single trial of a binary experiment:

* where success occurs with probability $p$ 

* and failure with probability ${1-p}$

:::

:::{.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false
#| warning: false

# Define probabilities
p_success <- 0.7
p_failure <- 1 - p_success

# Create data frame
df <- data.frame(
  Outcome = c("Success", "Failure"),
  Probability = c(p_success, p_failure)
)

# Plot
ggplot(df, aes(x = Outcome, y = Probability, group = Outcome)) +
  geom_bar(stat = "identity") +
  labs(title = "Probability Distribution of a Bernoulli Random Variable",
       x = "Outcome",
       y = "Probability") +
  theme_minimal(base_size = 14)

```

:::

::::

## Probability distribution

:::: columns

::: {.column}
$logit(p) = \log \frac{p}{1 - p}$

$y_i = Bernoulli(p) = \frac{p}{1 - p}$
:::

::: {.column}
**Mean of distribution** Probability (p) of observing an outcome

**Variance of observed responses** As observed responses approach 0 or 1, the variance of the distribution decreases
:::

::::

## The link function

If $μ = xβ$ is only true for normally distributed data, 
then, if this is not the case, we must use a transformation on the expected values:

$$g(μ) = xβ$$

where $g(μ)$ is the link function.

This allows us to relax the normality assumption.

## The logit link

For binary data, the link function is called the logit:

$$ logit(p) = \log \frac{p}{1 - p} $$



The log of the odds $(p / (1 - p))$


## The logit link

$$ logit(p) = \log \frac{p}{1 - p} $$

:::{.incremental}

- The odds put our expected values on a 0 to +Inf scale.

- The log transformation puts our expected values on a -Inf to +Inf scale.

- Now, the expected values can be linearly related to the linear predictor.

:::

## Probability, odds, logit-odds

::::{.columns}

:::{.column width="60%"}

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false

# Generate a sequence of independent variable values
independent_variable <- seq(-10, 10, by = 0.5)

# Define a function to calculate probabilities
calculate_probability <- function(x) {
  probability <- 1 / (1 + exp(-x))
  return(probability)
}

# Define a function to calculate odds
calculate_odds <- function(probability) {
  odds <- probability / (1 - probability)
  return(odds)
}

# Define a function to calculate log odds
calculate_log_odds <- function(odds) {
  log_odds <- log(odds)
  return(log_odds)
}

# Calculate probabilities, odds, and log odds
probabilities <- sapply(independent_variable, calculate_probability)
odds <- sapply(probabilities, calculate_odds)
log_odds <- sapply(odds, calculate_log_odds)

# Plot the relationships
par(mfrow = c(1, 3), mar = c(5, 5, 2, 2))
plot(independent_variable, probabilities, type = "l", col = "blue", xlab = "Independent Variable", ylab = "Probability", main = "Change in Probability", ylim = c(0, 1))
plot(independent_variable, odds, type = "l", col = "red", xlab = "Independent Variable", ylab = "Odds", main = "Change in Odds")
plot(independent_variable, log_odds, type = "l", col = "green", xlab = "Independent Variable", ylab = "Log Odds", main = "Change in Log Odds")


```

:::

:::{.column width="40%"}

- The odds put our expected values on a 0 to +Inf scale.

- The log transformation puts our expected values on a -Inf to +Inf scale.

- Now, the expected values can be linearly related to the linear predictor.

:::

::::


## Example

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false

mayfly_glm <- glm(Occupancy ~ CCU, family = binomial(link = "logit"), data = Mayflies)

summary(mayfly_glm)

```

Each coefficient corresponds to a predictor variable, indicating the change in the **log odds** of the response variable associated with a one-unit change in that predictor, holding other predictors constant.

## Interpret the model

:::{.incremental}

- Interpretation of coefficients involves exponentiating them to obtain odds ratios.

- An odds ratio greater than 1 implies higher odds of the event occurring with an increase in the predictor.

- An odds ratio less than 1 implies lower odds of the event occurring with an increase in the predictor.

:::

## Intepret the model

| Probability | Odds                              | Log Odds | Verbiage               |
|-------------|-----------------------------------|-----------------|------------------------|
| $p=.95$     | $\frac{95}{5} = \frac{19}{1} = 19$ |      2.94           | 19 to 1 odds for      |
| $p=.75$     | $\frac{75}{25} = \frac{3}{1} = 3$  |       1.09          | 3 to 1 odds for       |
| $p=.50$     | $\frac{50}{50} = \frac{1}{1} = 1$  |        0         | 1 to 1 odds           |
| $p=.25$     | $\frac{25}{75} = \frac{1}{3} = 0.33$ |    -1.11            | 3 to 1 odds against   |
| $p=.05$     | $\frac{95}{5}  = \frac{1}{19} = 0.0526$ |   -2.94          | 19 to 1 odds against  |

## Logit odds

When we use a binomial model, we produce the 'log-odds', this produces a fully unconstrained linear regression as anything less than 1, can now occupy an infinite negative value -∞ to ∞.


$$\log\left(\frac{p}{1-p}\right)	=	\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$$

:::{.fragment}

$$\frac{p}{1-p}	=	e^{\beta_{0}}e^{\beta_{1}x_{1}}e^{\beta_{2}x_{2}}$$

:::

:::{.fragment}

- We can interpret $\beta_{1}$ and $\beta_{2}$ as the increase in the log odds for every unit increase in $x_{1}$ and $x_{2}$. 

- We could alternatively interpret $\beta_{1}$ and $\beta_{2}$ using the notion that a one unit change in $x_{1}$ as a percent change of $e^{\beta_{1}}$ in the odds. 

:::

##

:::::{.columns}

::::{.column}

For the intercept: 

- The estimate as log odds is 5.102.

- Therefore the odds are 164

- Over 99% probability of observing mayfly when CCU = 0


::::

::::{.column}

```{r}
summary(mayfly_glm)
```

::::

:::::


##

:::::{.columns}

::::{.column}


For the predictor variable `CCU`:

- The estimate is -3.051.

- This means that for every one unit increase in `CCU`, the log odds of the response variable **decreases by 3.051**, holding all other predictors constant.


::::

::::{.column}

```{r}
summary(mayfly_glm)
```

::::

:::::

##

:::::{.columns}

::::{.column}

Now, let's interpret the coefficient for `CCU` using the odds ratio interpretation:

:::{.incremental}

- The odds ratio associated with `CCU` is calculated as $e^{\beta_{\text{CCU}}} = e^{-3.051}$.

- Therefore, $e^{\beta_{\text{CCU}}} \approx 0.048$.

- This means that for every one unit increase in `CCU`, the odds of the response variable decrease by a multiple of 0.048

:::

::::

::::{.column}

```{r}
#| eval: true
#| echo: true

broom::tidy(mayfly_glm, exponentiate = T)
```

::::

:::::


## Challenge


* Work with the odds and probability calculators

* Check how comfortable you are with additive log-odds, multiplicative odds and changing probability

```{r}
#| label: ex-odds-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```


## Model fit

For a simple Bernoulli/Binary GLM there are only a few checks that apply:

```{r}
#| eval: true
#| echo: true
#| fig-height: 6
#| layout-ncol: 3
#| message: false

# Calculate and plot "deviance" residuals
dresid <- resid(mayfly_glm, type = "deviance")
hist(dresid)

# Plot leverage
plot(mayfly_glm, which = 2)

# Plot Cook's distance
plot(mayfly_glm, which = 4)

```

## Model fit

Residuals in binary glms will never be normally distributed - so a "residuals vs fitted plot" provides little insight.

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false

plot(mayfly_glm, which = 1)

```


## DHARMa fit

- [DHARMa](https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/) works by **simulating residuals** from the fitted model, comparing them with observed residuals, and providing diagnostic plots to assess model adequacy and identify potential issues like heteroscedasticity or influential data points.

```{r}
#| echo: true
#| eval: true
library(DHARMa)
plot(simulateResiduals(mayfly_glm))

```


## Prediction


- `predict()` function for GLMs computes predicted values based on the fitted model.

- For existing data, it predicts values for each observation in the original dataset.

- For new data, it predicts values for a specified new dataset, allowing for predictions on unseen data.


```{r}
#| eval: false
#| echo: true
#| message: false

# Predictions for existing data
predict(mayfly_glm, type = "response")

```


```{r}
#| eval: false
#| echo: true
#| message: false

# Predictions for new data
new_CCU<- data.frame(CCU = c(0,1,2,3,4,5))
predict(mayfly_glm, newdata = new_CCU, type = "response")

```

## Emmeans

The `emmeans` package in R computes estimated marginal means (EMMs) it can also provides post-hoc comparisons and contrasts for fitted models.


- This specific function call below computes EMMs for the predictor variable CCU at specific values (0 to 5) and provides them on the response scale.

```{r}

emmeans::emmeans(mayfly_glm, 
                 specs = ~ CCU, 
                 at=list(CCU=c(0:5)), 
                 type='response') 


```

## Prediction plots with emmeans

We can use `emmeans` package to compute estimated marginal means (EMMs) from a fitted GLM, convert them into a tibble format for visualization with ggplot2, overlay the means with uncertainty ribbons and add them on a scatter plot of the original data.

::::{.columns}

:::{.column width="60%"}

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false
means <- emmeans::emmeans(mayfly_glm, 
                 specs = ~ CCU, 
                 at=list(CCU=c(0:5)), 
                 type='response') |> 
  as_tibble()

ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()+
    geom_ribbon(data = means,
              aes(x = CCU,
                  y = prob,
                  ymin = asymp.LCL,
                  ymax = asymp.UCL),
              alpha = .2)+
  geom_line(data = means,
            aes(x = CCU,
                y = prob)) +  # regression line
  labs(title = "Logit-link Binomial Regression",
       x = "CCU", y = "Occupancy")+
  theme_classic(base_size = 14)

```

:::

:::{.column width="40%"}


```{r}
#| eval: false
#| echo: true
#| fig-height: 7
#| message: false
means <- emmeans::emmeans(mayfly_glm, 
                 specs = ~ CCU, 
                 at=list(CCU=c(0:5)), 
                 type='response') |> 
  as_tibble()

ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()+
    geom_ribbon(data = means,
              aes(x = CCU,
                  y = prob,
                  ymin = asymp.LCL,
                  ymax = asymp.UCL),
              alpha = .2)+
  geom_line(data = means,
            aes(x = CCU,
                y = prob)) +  # regression line
  labs(title = "Logit-link Binomial Regression",
       x = "CCU", y = "Occupancy")+
  theme_classic(base_size = 14)

```

:::

::::

## Exercise


* Can you recreate the binomial glm for mayfly~ccu data?

```{r}
#| label: ex-ccu-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```



## Challenge

```{r}
#| label: ex-malaria-timer
countdown::countdown(
  minutes = 30,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

:::: columns

::: {.column}

![](images/warbler.jpeg){fig-align="center" fig-alt="Seychelles warbler" width=60%}
:::

:::{.column}

<br><br>

* Using the malaria dataset fit a binomial glm to look at the predictors of malaria in the Seychelles warbler

* Check the model fit

* Make predictions

* Produce a suitable figure

:::

::::


# GLMs for Binomial data {background-color="#D9DBDB"}

## Proportion data and GLM

Sometimes, count data aligns more closely with logistic regression methodologies than it initially appears.

We're not looking at typical count data when measuring the number of occurrences with a known total sample size.

Imagine, for example, we're studying the prevalence of a native underbrush species across various forest plots. We assess 15 different plots, each meticulously surveyed for the presence of this underbrush, counting the number of square meters where the underbrush is present versus absent within a standard area:

$$\frac{M^2~\text{with native underbrush (successes)}}{\text{Total surveyed area in}~M^2~(\text{trials})}$$

Bound between zero and one


## Binomial distribution

:::: columns

::: {.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false

# Define a range of trials
trial_sizes <- c(5, 10, 20, 50, 100)

# Define the probability of success
p_success <- 0.7

# Generate the binomial distribution data
binom_data <- map_df(trial_sizes, ~tibble(
  trials = .x,
  success = 0:.x,
  probability = dbinom(0:.x, .x, p_success)
)) %>%
  mutate(trials = factor(trials, levels = trial_sizes)) # This ensures the plots are ordered correctly

# Plot
ggplot(binom_data, aes(x = success, y = probability, color = trials)) +
  geom_line() + # Use geom_point() if you prefer dots
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Change in Binomial Distribution with Number of Trials",
       x = "Number of Successes",
       y = "Probability",
       color = "Number of Trials") +
  theme_minimal()

```

::: 


:::{.column}


$logit(p) = \log \frac{p}{1 - p}$

$y_i = Binomial(n,p)$

* It is the collection of Bernoulli trials for the same event

* It is represented by the number of Bernoulli trials $n$

* It is also the probability of an event in each trial $p$

:::

::::

## Binomial: Example

:::: columns

::: {.column}

![](images/Tribolium castaneum.jpg){fig-align="center" fig-alt="Tribolium" width=50%}
:::

:::{.column}

This small dataset is from an experiment looking at mortality in batches of flour exposed to different doses of pesticides.

The question of interest is: 

> How does pesticide dose affect mortality in the flour beetle *Tribolium castaneum*

:::

::::

## The data

```{r}
#|echo: true
#|message: false

beetles <- read_csv("../data/beetles.csv")
beetles
```

## Preparing the data

As well as the numbers killed, we need the numbers that remain alive

```{r}
#|echo: true
#|message: false

beetles <- beetles |> 
  rename("dead" = Number_killed,
         "trials" = Number_tested) |> 
  mutate("alive" = trials-dead)

beetles

```


## Binomial VS Bernoulli Keypoints!

Bernoulli deals with the outcome of the single trial of the event, whereas Binomial deals with the outcome of the multiple trials of the single event.

Bernoulli is used when the outcome of an event is required for only one time, whereas the Binomial is used when the outcome of an event is required multiple times.

## Binomial glm

The GLM of the binomial counts will analyse the number of *each* batch of beetles killed, while taking into account the size of each group

```{r}
#|echo: true
#|message: false

#cbind() creates a matrix of successes and failures for each batch

beetle_glm <- glm(cbind(dead, alive) ~ Dose, family = binomial(link = "logit"), data = beetles)

summary(beetle_glm)

```

## Weights

An alternative method is to analyse the proportion of beetles killed and including the number of trials with the `weight` argument

```{r}
#|echo: true
#|message: false


beetle_glm_weights <- glm(Mortality_rate ~ Dose, weights = trials, family = binomial(link = "logit"), data = beetles)

summary(beetle_glm_weights)

```


## 

```{r}
#| echo: true
#| eval: true
#| message: false

# Alternatively, using package-specific functions
# Example with 'broom' package
library(broom)
tidy(beetle_glm, conf.int = T)

```


## Table

This function has automatically applied the exponentiate to produce the odds and odds ratio:

```{r}
#| echo: true
#| eval: true
#| message: false

library(sjPlot)

tab_model(beetle_glm)

```

## Predictions


```{r}
#| echo: true
#| eval: true
#| message: false
emmeans::emmeans(beetle_glm, 
                 specs = ~ Dose, 
                 at=list(Dose=c(40, 50, 60, 70, 80)), 
                 type='response') 
```

## Predictions plot

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false

means <- emmeans::emmeans(beetle_glm, 
                 specs = ~ Dose, 
                 at=list(Dose=c(40:80)), 
                 type='response')  |> 
  as_tibble()

ggplot(beetles, aes(x=Dose, y=Mortality_rate)) + geom_point()+
    geom_ribbon(data = means,
              aes(x = Dose,
                  y = prob,
                  ymin = asymp.LCL,
                  ymax = asymp.UCL),
              alpha = .2)+
  geom_line(data = means,
            aes(x = Dose,
                y = prob)) +  # regression line
  labs(title = "Logit-link Binomial Regression",
       x = "Dose", y = "Mortality")+
  theme_classic(base_size = 14)

```



## Overdispersion

If the residual deviance of the model is > the residual degrees of freedom - we consider our binomial models to have **overdispersion**. 

This means the variance is greater than predicted by the binomial distribution

Overdispersion > 1.5 can be accounted for with the *quasi-binomial* glm

```{r}
summary(beetle_glm_weights)
```

## Overdispersion

:::: columns

::: {.column}

**Binomial**
$P(X = s) = C(n, s) \cdot p^s \cdot (1 - p)^{n - s}$

:::

:::{.column}

**Quasibinomial**
$P(X = s) = C(n, s) \cdot p(p+k\theta)^{s-1} \cdot (1 - pk\theta)^{n - s}$

:::

::::

Where:

- $n$ is the total number of trials of an event.
- $s$ corresponds to the number of times an event should occur.
- $p$ is the probability that the event will occur.
- $(1 - p)$ is the probability that the event will not occur.
- $C$ term represents combinations, calculated as $C(n, s) = \frac{n!}{s!(n - s)!}$, representing the number of ways to choose $s$ successes out of $n$ trials.

- $\theta$ term describes additional variance outside of the Binomial distribution

## Fitting the quasibinomial model

```{r}
#|echo: true
#|message: false

summary(beetle_glm)

```

```{r}
#|echo: true
#|message: false

beetle_quasiglm <- glm(cbind(dead, alive) ~ Dose, family = quasibinomial(link = "logit"), data = beetles)

summary(beetle_quasiglm)

```


## Fitting the quasibinomial model

Standard Error, Confidence intervals and p-values WILL change.

Estimates remain the same

```{r}
#| eval: true
#| echo: false
#| fig-height: 6
#| message: false

means <- emmeans::emmeans(beetle_quasiglm, 
                 specs = ~ Dose, 
                 at=list(Dose=c(40:80)), 
                 type='response')  |> 
  as_tibble()

ggplot(beetles, aes(x=Dose, y=Mortality_rate)) + geom_point()+
    geom_ribbon(data = means,
              aes(x = Dose,
                  y = prob,
                  ymin = asymp.LCL,
                  ymax = asymp.UCL),
              alpha = .2)+
  geom_line(data = means,
            aes(x = Dose,
                y = prob)) +  # regression line
  labs(title = "Logit-link Binomial Regression",
       x = "Dose", y = "Mortality")+
  theme_classic(base_size = 14)

```

## Challenge


:::: columns


::: {.column width="60%"}

On the afternoon of January 28th, 1986 the space shuttle Challenger experienced a critical failure and broke apart in mid air, resulting in the deaths of all seven crewmembers.

An investigation discovered critical failures in all six of the O-rings in the solid rocket booster.

The question of interest is:

> How does temperature affect the probability of o-ring failure?

> What was the probability of all o-rings failing when the temperature was 36 degrees F

:::

::: {.column width="40%"}

```{r}
#| label: ex-challenger-timer
countdown::countdown(
  minutes = 20,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

![](images/space-shuttle.jpg){fig-align="center" fig-alt="Challenger" width=60%}

:::

::::


# GLMs for count data {background-color="#D9DBDB"}

## Poisson

:::: columns

::: {.column}

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| message: false

# Define a range of lambda values (average rate of success)
lambda_values <- c(1, 4, 10, 20, 50)

# Generate the Poisson distribution data
poisson_data <- map_df(lambda_values, ~tibble(
  lambda = .x,
  events = 0:75, # Assuming a reasonable range for visualization
  probability = dpois(0:75, .x)
)) |> 
  mutate(lambda = factor(lambda, levels = lambda_values)) # For ordered plotting

# Plot
ggplot(poisson_data, aes(x = events, y = probability, color = lambda)) +
  geom_line(linewidth = 1.5) +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Change in Poisson Distribution with Different Lambda",
       x = "Number of Events",
       y = "Probability",
       color = "Lambda") +
  theme_minimal(base_size = 14)

```

::: 

::: {.column}

Count or rate data are ubiquitous in the life sciences (e.g number of parasites per microlitre of blood, number of species counted in a particular area). These type of data are **discrete** and **non-negative**.

A useful distribution to model abundance data is the **Poisson** distribution: 

a discrete distribution with a single parameter, λ (lambda), which defines both the mean and the variance of the distribution.

:::


::::

## GLM

::: {.incremental}

Recall the simple linear regression case (i.e a GLM with a Gaussian error structure and identity link). For the sake of clarity let's consider a single explanatory variable where $\mu$ is the mean for *Y*:

$\mu_i = \beta0 + \beta_1x1 + \beta_2x2 + ... +\beta_nxn$

$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$

* The mean function is **unconstrained**, i.e the value of $\beta_0 + \beta_1X$ can range from $-\infty$ to $+\infty$. 

:::

## Poisson


* If we want to model count data we therefore want to **constrain** this mean to be positive only. 

:::{.incremental}

* Mathematically we can do this by taking the **logarithm** of the mean (the log is the default link for the Poisson distribution). 

* We then assume our count data variance to be Poisson distributed (a discrete, non-negative distribution), to obtain our Poisson regression model (to be consistent with the statistics literature we will rename $\mu$ to $\lambda$):

:::

## GLM Poisson

![](images/poisson_regression.webp){fig-align="center" fig-alt="Poisson equation" width=80%}

## Poisson limitations

The **Poisson** distribution has the following characteristics:

::: {.incremental}

* **Discrete** variable (integer), defined on the range $0, 1, \dots, \infty$.

* A single ***rate*** parameter $\lambda$, where $\lambda > 0$.

* **Mean** = $\lambda$  

* **Variance** = $\lambda$

:::


## Poisson: Case study

:::: columns

::: {.column}

![](images/cuckoo.jpg){fig-align="center" fig-alt="Cuckoo" width=50%}
:::

:::{.column}

In a study by [Kilner *et al.* (1999)](http://www.nature.com/nature/journal/v397/n6721/abs/397667a0.html), the authors
studied the begging rate of nestlings in relation to total mass of the brood of **reed warbler chicks** and **cuckoo chicks**.
The question of interest is:

> How does nestling mass affect begging rates between the different species?

:::

::::

##

This model is inadequate. It is predicting **negative** begging calls *within* the 
range of the observed data, which clearly does not make any sense.

```{r}
#| eval: false
#| echo: true
#| fig-height: 8
#| message: false

cuckoo_lm <- lm(Beg ~ Mass + Species + Mass:Species, data = cuckoo)
```


```{r}
#| include: FALSE

cuckoo <- read_csv("../data/cuckoo.csv")

```

```{r}
#| eval: true
#| echo: false
#| fig-height: 7
#| message: false

cuckoo_lm <- lm(Beg ~ Mass + Species + Mass:Species, data = cuckoo)

broom::augment(cuckoo_lm, type.predict = "response") %>% 
ggplot(aes(x=Mass, y=.fitted, colour=Species)) + 
  geom_point() +
  geom_line()+
  geom_hline(yintercept = 0, linetype = "dashed")+
  scale_colour_manual(values=c("green3","turquoise3"))+
  scale_x_continuous(limits = c(0,40)) +
  theme_minimal()



```


## Diagnostics

Let us display the model diagnostics plots for this linear model.

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-height: 8

plot(cuckoo_lm, which=2)

plot(cuckoo_lm, which=3)
```


* Curvature

* Funnelling effect


## Biological data and distributions

We can look at the kurtosis and skewness of our univariate variable - Begging: 

```{r}
#| echo: true
#| fig-height: 7

descdist(cuckoo$Beg, boot = 500, discrete = T)

```

## Univariate distributions

::::{.columns}

:::{.column}

```{r}
#| echo: true
#| eval: false
#| fig-height: 7
#| warning: false

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fp <- fitdist(cuckoo$Beg, "pois")
fn <- fitdist(cuckoo$Beg, "norm")
fnb<- fitdist(cuckoo$Beg, "nbinom")
plot.legend <- c("normal", "poisson", "negative binomial")
denscomp(list(fn, fp, fnb), legendtext = plot.legend)
qqcomp(list(fn, fp, fnb), legendtext = plot.legend)
```

:::

:::{.column}

```{r}
#| echo: false
#| fig-height: 7
#| warning: false

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fp <- fitdist(cuckoo$Beg, "pois")
fn <- fitdist(cuckoo$Beg, "norm")
fnb<- fitdist(cuckoo$Beg, "nbinom")
plot.legend <- c("normal", "poisson", "negative binomial")
denscomp(list(fn, fp, fnb), legendtext = plot.legend)
qqcomp(list(fn, fp, fnb), legendtext = plot.legend)
```

:::

::::


## Poisson model

We should therefore try a different model structure.

The response variable in this case is a classic **count data**: **discrete** and bounded below by zero (i.e we cannot have negative counts). We will therefore try a **Poisson model** using the canonical **log** link function for the mean:


* λ varies with x (mass) which means residual variance will also vary with 
x, which means that we just relaxed the homogeneity of variance assumption!

* Predicted values will now be integers instead of fractions

* The model will never predict negative values (Poisson is strictly positive)

$$
    \log{\lambda} = \beta_0 + \beta_1 M_i + \beta_2 S_i + \beta_3 M_i S_i
$$

## Poisson model

$$
    \log{\lambda} = \beta_0 + \beta_1 M_i + \beta_2 S_i + \beta_3 M_i S_i
$$

where $M_i$ is nestling mass and $S_i$ a **dummy** variable

$S_i = \left\{\begin{array}{ll}1 & \text{if } i \text{ is warbler},\\0 & \text{otherwise}.\end{array}\right.$

The term $M_iS_i$ is an **interaction** term. Think of this as an additional explanatory variable in our model.
Effectively it lets us have **different** slopes for different species (without an interaction term we assume that
both species have the same slope for the relationship between begging rate and mass, and only the intercept differ).

## Regression lines

The mean regression lines for the two species look like this:

::: {.incremental}

* **Cuckoo** ($S_i=0$)

* $\begin{aligned}\log{\lambda} & = \beta_0 + \beta_1 M_i + (\beta_2 \times 0)  + (\beta_3 \times M_i \times 0)\\\log{\lambda} & = \beta_0 + \beta_1 M_i\end{aligned}$
    


* **Warbler** ($S_i=1$)

* $\begin{aligned}\log{\lambda} & = \beta_0 + \beta_1 M_i + (\beta_2 \times 1)  + (\beta_3 \times M_i \times 1)\\\log{\lambda} & = \beta_0 + \beta_1 M_i + \beta_2 + \beta_3M_i\\\end{aligned}$

:::


## Fit the Poisson model

Fit the model with the interaction term in R:

```{r}
#| echo: true

cuckoo_glm1 <- glm(Beg ~ Mass + Species + Mass:Species, data=cuckoo, family=poisson(link="log"))

summary(cuckoo_glm1)
```

> Note there appears to be a negative interaction effect for Species:Mass, indicating that Begging calls do not increase with mass as much as you would expect for Warblers as compared to Cuckoos.


## Exercise 


* Fit a Poisson model to the cuckoo data

* Look at the residual plots - how have they changed? 


```{r}
#| label: ex-pois-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```


## Model summary

```{r}
#| echo: true

summary(cuckoo_glm1)

```

## Residuals

```{r}
#| echo: true
#| layout-ncol: 2

plot(cuckoo_glm1, which=2)
plot(cuckoo_glm1, which=3)

```



## Poisson model check

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-height: 8

plot(cuckoo_glm1, which=2)

plot(cuckoo_glm1, which=3)
```

## VIF

::::{.columns}

:::{.column}

VIF, or Variance Inflation Factor, measures the extent of multicollinearity among independent variables in regression analysis.


High multicollinearity, indicated by elevated VIF values, can distort regression coefficients and inflate standard errors, undermining the reliability of the model's predictions

```{r}
#| eval: true
#| echo: true
#| fig-height: 8


car::vif(cuckoo_glm1)

```

:::

:::{.column}

```{r}
#| eval: true
#| echo: true
#| fig-height: 8

check_model(cuckoo_glm1, check = "vif")

```

:::

::::

## Mean-centering

True VIF is highly problematic 

- Mean centering can remove apparent VIF as means become zero

- This often decreases potential correlation

```{r}
#| eval: true
#| echo: true

cuckoo$mass.c <- cuckoo$Mass - mean(cuckoo$Mass, na.rm =T)

cuckoo_glm2 <- glm(Beg ~ mass.c + Species + mass.c:Species, data=cuckoo, family=poisson(link="log"))

summary(cuckoo_glm2)

```


## Exercise 

<br><br>


* `cuckoo$mass.c <- cuckoo$Mass - mean(cuckoo$Mass, na.rm =T)`

* Mean center the mass variable and recheck VIF.


```{r}
#| label: ex-vif-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

## Model selection

For a term with only one degree of freedom $z^2$ = the chi-square value

The likelihood ratio test allows us to determine whether we can remove an interaction term

```{r}
#| eval: true
#| echo: true

# For a fixed  mean-variance model we use a Chisquare distribution
drop1(cuckoo_glm2, test = "Chisq")

```


## Offset - for rates

In Poisson GLMs, using an offset allows us to model rates rather than counts, by including the logarithm of the exposure variable as an offset, ensuring that the model accounts for the differing exposure levels across observations.

```{r}
#| eval: true
#| echo: true

cuckoo_glm3 <- glm(Call_Total ~ mass.c + Species + mass.c:Species, data=cuckoo, offset = log(Mins), family=poisson(link="log"))

summary(cuckoo_glm3)

```


## Poisson limitations

So for the Poisson regression case we assume that the mean and variance are the same.
Hence, as the mean *increases*, the variance *increases* also (**heteroscedascity**).
This may or may not be a sensible assumption so watch out! Just because a Poisson distribution *usually* fits well for count data, doesn't mean that a Gaussian distribution *can't* always work.

Recall the link function between the predictors and the mean and the rules of logarithms (if $\log{\lambda} = k$(value of predictors), then $\lambda = e^k$):


## Overdispersion

When the residual deviance is higher than the residual degrees of freedom, we say that the model is overdispersed. This situation is mathematically represented as:

$$
\phi = \frac{\text{Residual deviance}}{\text{Residual degrees of freedom}}
$$

Overdispersion occurs when the variance in the data is even higher than the mean, indicating that the Poisson distribution might not be the best choice. This can be due to many reasons, such as the presence of many zeros, many very high values, or missing covariates in the model.

## Overdispersion and what to do about it

| Causes of over-dispersion | What to do about it | 
|---------|:-----|
| Model mis-specification (missing covariates or interactions)    | Add more covariates or interaction terms   |
| Too many zeros ("zero inflation")    | Use a zero-inflated model  |
| Non-independence of observations       | Use a generalised mixed model with random effects to take non-independence into account    |
| Variance is larger than the mean      | Use a quasipoisson GLM if overdispersion = 2-15. Use a negative binomial GLM if > 15-20  |

: Overdispersion statistic values > 1


## Overdispersion and what to do about it

It may be true that you are initially unsure whether overdispersion is coming from zero-inflation, larger variance than the mean or both. 

You can fit models with the same dependent and independent variables and different error structures and use AIC to help determine the best model for your data

## Overdispersion in the cuckoo model

```{r}
#| eval: true
#| echo: true

summary(cuckoo_glm2)

```

## Quasi-Poisson GLM 

The systematic part and the link function remain the same

$$
    \log{\lambda} = \beta_0 + \beta_1 x_i
$$

*phi*$\phi$ is a dispersion parameter, it will not affect estimates but will affect *significance*. Standard errors are multiplied by $\sqrt\phi$ 

$$
Y_i = Poisson(\phi \lambda_i)
$$


Where:

- $Yi$ is the response variable.
- $\phi$ is the dispersion parameter, which adjusts the variance of the distribution

Confidence intervals and p-values WILL change.

## Exercise


* Fit a quasipoisson model to the cuckoo data

```{r}
#| eval: true
#| echo: true

cuckoo_glm2 <- glm(Beg ~ mass.c + Species + mass.c:Species, data=cuckoo, family=quasipoisson(link="log"))
```

* Look at the residual plots - how have they changed? 

* Look at the SE and p-values - how have they changed

* Calculate new 95% confidence intervals


## F distributions

The presence of overdispersion suggested the use of the F-test for nested models. We will test if the interaction term can be dropped from the model.

Though here for a term with only one degree of freedom $t^2$ = the F distribution with denominator degree of freedom

```{r}
#| eval: true
#| echo: true

drop1(cuckoo_glm2, test = "F")

```

## Zero-inflation

"Zero-inflation" refers to the problem of "too many zeros". 

The dependent variable contains more zeros than would be expected under the standard statistical distribution for zero-bounded count data (Poisson or negative binomial). 

Zero-inflation is a common feature of count datasets. Large numbers of zeros can arise because

1) There was nothing there to count (**true zeros**)

2) Things were there but missed (**false zeros**)

## Zero-inflation

:::: columns

::: {.column}

| Model | Acronym/Abbr | Source of Overdispersion |
|---------|:-----|:-----|
| Zero-inflated Poisson | ZIP | Zeros |
| Negative Binomial | NegBin | Positive integers |
| Zero-inflated Negative Binomial | ZINB | Zeros + Positive integers |

:::

::: {.column}

![](images/zip.png){fig-align="center" fig-alt="Zip" width=100%}

:::

::::

## ZIP in R

In a ZIP model we define two halves of the model; the first half models the count process (**i.e. a Poisson GLM explaining non-zero integers**) and the second half models the binomial (**i.e. explains the zeros**).

```{r}
#| eval: false
#| echo: true

install.packages("pscl")
```

```{r}
#| eval: false
#| echo: true
library(pscl)
zip1 <- zeroinf(y ~ x1 + x2 | x1 + x2,
                dist = "poisson",
                link = "logit",
                data = dataframe)
```

## Alternative model design

You can change the zero-inflation model's link function and use AIC to decide on the best model

```{r}
#| eval: false
#| echo: true

# zeros modelled with a constant
y ~ x1 + x2

# zeros modelled with the same variables
y ~ x1 + x2 | x1 + x2

# zeros modelled with different variables
y ~ x1 + x2 | z1 +z2

```


## Checking for zero-inflation


```{r}
#| eval: true
#| echo: true

hist(cuckoo$Beg)

```


```{r}
#| eval: true
#| echo: true

check_zeroinflation(cuckoo_glm2)

```

## Fit the model

Fit the model 

```{r}
#| echo: true

cuckoo_zip <- zeroinfl(Beg ~ mass.c + Species + mass.c:Species| 
                   mass.c + Species + mass.c:Species,
                dist = "poisson",
                link = "logit",
                data = cuckoo)

summary(cuckoo_zip)

```


## Model validation

Model validation is not straightforward for zero-inflated models


::::{.columns}

:::{.column}

```{r}
#| echo: true

sresid <- residuals(cuckoo_zip, type = "pearson")

pred <- predict(cuckoo_zip)


```

:::

:::{.column}

```{r}
#| echo: true

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

hist(sresid)
plot(sresid ~ pred)
qqnorm(sresid)
qqline(sresid, col = "red")

```
:::

::::


## Negative Binomial

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| fig-width: 10
#| message: false

##Negative Binomial Distribution (Varying Shape Parameter r)
#In the negative binomial distribution:
  
#  Shape Parameter (r): Represents the number of successes required before the experiment is stopped.
#Probability of Success (p): Represents the probability of success in each Bernoulli trial.
#As r increases:
  
#  The distribution becomes more skewed to the right.
#The average number of failures before success increases.
#The variance of the distribution also increases, indicating greater variability.

# Function to generate negative binomial distribution data
generate_nbinom_data <- function(r, p) {
  tibble(failures = 0:(10*r), 
         probability = dnbinom(0:(10*r), size = r, prob = p))
}

# Define range of shape parameter values (r)
r_values <- rep(c(2, 5, 10), times = 3)  # Number of successes

# Fixed probability of success
p_fixed <- rep(c(0.2, 0.4, 0.6), each = 3)  

# Generate negative binomial distribution data for different r values
nbinom_data <- map2_df(r_values, p_fixed, ~generate_nbinom_data(r = .x, p = .y) 
                      %>% mutate(r = as.factor(.x)) %>% mutate(p = as.factor(.y))) 

# Plot for negative binomial distribution
ggplot(nbinom_data, aes(x = failures, y = probability, color = r, group = interaction(r,p))) +
  geom_line() +
  scale_color_brewer(palette = "Dark2")+
  labs(title = "Negative Binomial Distribution with Varying Shape Parameter (r)",
       x = "Number of Failures",
       y = "Probability",
       color = "Number of Successes (r)") +
  theme_minimal(base_size = 14)+
  facet_wrap(~p)+
  gghighlight::gghighlight()

```

## Negative Binomial

Negative binomial GLMs are favored when overdispersion is high.

It has two parameters, $μ$ and $\theta$. $\theta$ controls for the dispersion parameter (smaller $\theta$ indicates higher dispersion). It corresponds to a combination of two distributions (Poisson and gamma).

It assumes that the $Y_i$ are Poisson distributed with the mean $μ_i$ assumed to follow a gamma distribution:

$$
E(Y_i) = μ_i \\
\text{Var}(Y_i) = μ_i + μ_i^2/k
$$


## Fitting a negative binomial in R

Negative binomial is not in the `glm()` function. We need the `MASS` package:

Suitable for strongly over-dispersed zero-bounded integer data.

```{r}
#| eval: false
#| echo: false
#| message: false

library(MASS)

model <- glm.nb(y ~ x1 + x2, link = "log", data = dataframe)

```


## Negbin: Example

:::: columns

::: {.column}

![](images/3 Spined Stickleback.jpg){fig-align="center" fig-alt="Stickleback" width=50%}
:::

:::{.column}

The example is an experiment measuring the effect of the parasitic tapeworm *Schistocephalus solidus* infection on the susceptibility of infection from a second parasite, the trematode *Diplostomum pseudospathaceum*

The question of interest is:

> How does Treatment/Infection status affect trematode counts?

:::

::::


## Exercise

```{r}
#| label: ex-stickpois-timer
countdown::countdown(
  minutes = 10,
  color_border = "#00AEEF",
  color_text = "#00AEEF",
  color_running_text = "white",
  color_running_background = "#00AEEF",
  color_finished_text = "#00AEEF",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```


- Determine whether the poisson model is suitable for this dataset?

```{r}
#| eval: true
#| echo: false
#| fig-height: 8
#| fig-width: 10
#| message: false

parasite <- read_csv("../data/parasite_exp.csv")

stick_poisson <- glm(Diplo_intensity ~ Treatment, family = "poisson", data = parasite)

```

## Univariate distribution

```{r}
#| echo: true
#| fig-height: 7
#| warning: false

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
fp <- fitdist(parasite$Diplo_intensity, "pois")
fn <- fitdist(parasite$Diplo_intensity, "norm")
fnb<- fitdist(parasite$Diplo_intensity, "nbinom")
plot.legend <- c("normal", "poisson", "negative binomial")
denscomp(list(fn, fp, fnb), legendtext = plot.legend)
qqcomp(list(fn, fp, fnb), legendtext = plot.legend)
```

## Check Poisson model

```{r}
#| echo: true
#| fig-height: 7
#| warning: false

summary(stick_poisson)

```


## Check zero-inflation

The model is underfitting zeros (but not by much). 

```{r}
#| echo: true
#| fig-height: 7
#| warning: false
check_zeroinflation(stick_poisson)

```

## Compare models

```{r}
#| echo: true
#| fig-height: 7
#| warning: false
# Quasilikelihood
stick_quasi <- glm(Diplo_intensity ~ Treatment, family = quasipoisson, data = parasite)

```


```{r}
#| echo: true
#| fig-height: 7
#| warning: false
# Neg bin
stick_nb <- glm.nb(Diplo_intensity ~ Treatment, link = "log", data = parasite)

```


```{r}
#| echo: true
#| fig-height: 7
#| warning: false
# Zero-inflated model
stick_zip <- zeroinfl(Diplo_intensity ~ Treatment| 
                   Treatment,
                dist = "poisson",
                link = "logit",
                data = parasite)


```

## Compare models

```{r}
#| echo: true
#| fig-height: 7
#| warning: false


AIC(stick_poisson)
AIC(stick_quasi)
AIC(stick_nb)
AIC(stick_zip)

```

## Comparison of model predictions

```{r}

colors <- c("Quasi" = "cyan", "NegBin" = "darkorange", "ZeroInfl" = "purple")

means_quasi <- emmeans::emmeans(stick_quasi, 
                 specs = ~ Treatment, 
                 type='response') |> 
  as_tibble()

means_nb <- emmeans::emmeans(stick_nb, 
                 specs = ~ Treatment, 
                 type='response') |> 
  as_tibble()

means_zip <- emmeans::emmeans(stick_zip, 
                 specs = ~ Treatment, 
                 type='response') |> 
  as_tibble()

ggplot(parasite, aes(x=Treatment, y=Diplo_intensity)) + 
  geom_jitter(width = .2,
              alpha = .4)+
 geom_pointrange(data = means_quasi,
                 aes(x = Treatment,
                     y = rate,
                     ymin = asymp.LCL, ymax = asymp.UCL,
                     colour = "Quasi"))+
geom_pointrange(data = means_nb,
                 aes(x = Treatment,
                     y = response,
                     ymin = asymp.LCL, ymax = asymp.UCL,
                     color = "NegBin"),
                position = position_nudge(x= .2))+
geom_pointrange(data = means_zip,
                 aes(x = Treatment,
                     y = emmean,
                     ymin = asymp.LCL, ymax = asymp.UCL,
                     color = "ZeroInfl"),
                position = position_nudge(x= -.2))+  
  theme_classic(base_size = 14)+
  scale_color_manual(values = colors)

```

# Simulate {background-color="#D9DBDB"}


# Survival {background-color="#D9DBDB"}

```{r}
#| eval: false
#| echo: false

model3 <- survreg((Surv(`Hours`, event) ~ source * supplementation), data = data2, dist = "weibull")
lin.pred <- predict(model3, type = "lp")[data2$event ==1]
log.resid <- log(data2$`Hours`[data2$event==1]) - lin.pred
car::qqPlot(exp(log.resid), dist = "weibull", shape = 1/model3$scale)
```

# Wrap-up {background-color="#D9DBDB"}

## Where next?


<br><br>

::: {.fragment}


* (Generalised) Linear **Mixed** Models

:::
::: {.fragment}
* Generalised Additive Models

:::


## Live demo!

<br><br>

::: {.fragment}

See `examples/example_04.R` for full code.

:::

## Exercise 4

Open `exercises/exercise_04.R` for prompts.

* Look at the ACF and PACF plots of the residuals from your previous GAM.

* Try fitting a GAMM model instead.

```{r}
#| label: ex-4-timer
countdown::countdown(
  minutes = 10,
  color_border = "#b20e10",
  color_text = "#b20e10",
  color_running_text = "white",
  color_running_background = "#b20e10",
  color_finished_text = "#b20e10",
  color_finished_background = "white",
  top = 0,
  margin = "1.2em",
  font_size = "2em"
)
```

. . . 

See `exercise_solutions/exercise_solutions_04.R` for full solution.

## Additional resources

* GLMs resource list: 

* Discovering Statistics - Andy Field

* An Introduction to Generalized Linear Models - Dobson & Barnett

* An Introduction to Statistical Learning with Applications in R - James, Witten, Hastie & Tibshirani

* Mixed Effects Models and Extensions in Ecology with R - Zuur, et al.



## 

::: columns
::: {.column}

<br>

{{< fa brands linkedin >}} [philip-leftwich](https://www.linkedin.com/in/philip-leftwich-117052155/)

{{< fa brands mastodon >}} [\@ecoevo.social\@PhilipLeftwich](https://ecoevo.social/@PhilipLeftwich)

{{< fa brands github >}} [PhilipLeftwich](https://github.com/Philip-Leftwich)

{{< fa globe >}} [philip.leftwich.github.io](https://philip.leftwich.github.io/)



:::
::: {.column}


:::
:::
